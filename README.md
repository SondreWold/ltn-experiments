# Approaching the Super GLUE benchmark with Logic Tensor Networks
Comparing standard CrossEntropy based training on the GLUE tasks to a LTN based approach using the SatAgg objective. 

## Why?

Why not?

## Datasets

MNLI, QNLI, SST-2, WNLI, RTE, CoLA

## Representing the tasks as First Order Logic

## Comparison with baseline models

## Lessons learned
