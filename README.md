# Approaching the GLUE benchmark with Logic Tensor Networks
Comparing standard CrossEntropy based training on the [GLUE](https://gluebenchmark.com/) tasks to a LTN based approach using the SatAgg objective. 

## Why?

Why not?

## Datasets

MNLI, QNLI, SST-2, WNLI, RTE, CoLA

## Representing the tasks as First Order Logic
TOOD

## Comparison with baseline models
TODO

## Lessons learned
TODO
